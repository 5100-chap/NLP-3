{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNl8G3vHkPSX"
   },
   "source": [
    "# **Maestría en Inteligencia Artificial Aplicada**\n",
    "\n",
    "## Curso: **Procesamiento de Lenguaje Natural**\n",
    "\n",
    "### Tecnológico de Monterrey\n",
    "\n",
    "### Prof Luis Eduardo Falcón Morales\n",
    "\n",
    "## Actividad Semana 5\n",
    "\n",
    "### **Vectores Embebidos de OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U69mHA6i201G"
   },
   "source": [
    "#### **Nombres y matrículas de los integrantes del equipo:**\n",
    "\n",
    "\n",
    "\n",
    "*   Elemento de lista\n",
    "*   Elemento de lista\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wCL2p6MA8NuT",
    "ExecuteTime": {
     "end_time": "2025-05-26T02:49:47.591663Z",
     "start_time": "2025-05-26T02:49:37.672356Z"
    }
   },
   "source": [
    "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# es un tokenizador que ayuda a dividr el texto en enunciados mediante un modelo no-supervisado.\n",
    "nltk.download('punkt')\n",
    "# para tener acceso a \"stopwords\" en varios idiomas.\n",
    "nltk.download('stopwords')\n",
    "# es un etiquetador de partes del discurso (POS) para inglés.\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "# es un recurso léxico para el inglés que agrupa palabras en conjuntos de sinónimos (synsets) y proporciona definiciones y ejemplos de uso.\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\it-\n",
      "[nltk_data]     user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\it-\n",
      "[nltk_data]     user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\it-user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\it-\n",
      "[nltk_data]     user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7wc107K-oIV4",
    "ExecuteTime": {
     "end_time": "2025-05-26T02:49:53.986024Z",
     "start_time": "2025-05-26T02:49:52.019931Z"
    }
   },
   "source": [
    "# Incluye las celdas necesarias para tu acceso a la API de OpenAI.\n",
    "import openai\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c34ZOnna3Gu"
   },
   "source": [
    "# **Pregunta - 1:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeNllxRdmeWg"
   },
   "source": [
    "Descarga los 3 archivos de Canvas y genera un nuevo DataFrame de Pandas con ellos.\n",
    "\n",
    "**Llama simplemente \"df\" a dicho DataFrame.**\n",
    "\n",
    "Los archivos los encuentras en Canvas: amazon5.txt, imdb5.txt, yelp5.txt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T_lyEFRkxzC6",
    "ExecuteTime": {
     "end_time": "2025-05-26T02:49:58.314285Z",
     "start_time": "2025-05-26T02:49:58.295340Z"
    }
   },
   "source": [
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "dfa = pd.read_csv('amazon5.txt', sep='\\t', names=['review', 'label'], header=None, encoding='utf-8')\n",
    "dfy = pd.read_csv('yelp5.txt', sep='\\t', names=['review', 'label'], header=None, encoding='utf-8')\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:50:06.762671Z",
     "start_time": "2025-05-26T02:50:06.750619Z"
    }
   },
   "source": [
    "# Abrimos el archivo imdb5.txt y lo procesamos por separado ya que no tiene el mismo formato que los otros archivos.\n",
    "with open('imdb5.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# dividimos cada línea en dos partes: el texto de la reseña y la etiqueta\n",
    "data = [line.rsplit(maxsplit=1) for line in lines]\n",
    "\n",
    "# convertimos la lista de listas en un DataFrame\n",
    "dfi = pd.DataFrame(data, columns=['review', 'label'])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:50:11.320986Z",
     "start_time": "2025-05-26T02:50:11.299538Z"
    }
   },
   "source": [
    "df = pd.concat([dfa, dfi, dfy], ignore_index=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "print(df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 2)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3-w1xMLYnm9b",
    "ExecuteTime": {
     "end_time": "2025-05-26T02:50:13.051461Z",
     "start_time": "2025-05-26T02:50:13.036706Z"
    }
   },
   "source": [
    "# Verifiquemos la información del DataFrame:\n",
    "df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  3000 non-null   object\n",
      " 1   label   3000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NfVUcYe1nubT",
    "ExecuteTime": {
     "end_time": "2025-05-26T02:50:16.558140Z",
     "start_time": "2025-05-26T02:50:16.536152Z"
    }
   },
   "source": [
    "# Y veamos sus primeros registros:\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfZZ0stLmWJN"
   },
   "source": [
    "# **Pregunta - 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F6JF5BommZ6"
   },
   "source": [
    "Realiza el proceso de limpieza. Aplica el preprocesamiento que consideres adecuado.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TsnvMp-7oYCM",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:34:43.189898Z",
     "start_time": "2025-05-25T17:34:43.183434Z"
    }
   },
   "source": [
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Definimos una lista de stopwords que no incluya las palabras negativas para evitar que se eliminen\n",
    "# en el preprocesamiento, ya que pueden ser importantes para la clasificación.\n",
    "negwords = ['no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "            'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn',\n",
    "            \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "            \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "mystopwords = set(stopwords.words('english')) - set(negwords)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:50:32.667529Z",
     "start_time": "2025-05-26T02:50:32.660181Z"
    }
   },
   "source": [
    "# Separamos nuestro dataset en dos variables, X e Y, donde X contiene las reseñas y Y contiene las etiquetas.\n",
    "X = df.review\n",
    "Y = df.label\n",
    "\n",
    "# Verificamos que la longitud de X e Y sean correctas.\n",
    "assert X.shape == (3000,)\n",
    "assert Y.shape == (3000,)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:34:44.686922Z",
     "start_time": "2025-05-25T17:34:44.680573Z"
    }
   },
   "source": [
    "# Definimos una función para limpiar el texto de las reseñas.\n",
    "def clean_tok(doc):\n",
    "    # Remplazamos los caracteres de puntuación por espacios.\n",
    "    doc = re.sub(r'[^A-Za-z0-9]', ' ', doc)\n",
    "    # Separamos el texto en tokens.\n",
    "    as_list = doc.split()\n",
    "    # eliminamos los caracteres no alfabéticos y convertimos a minúsculas.\n",
    "    as_list = [re.sub(r'[^A-Za-z]+', '', s).lower() for s in as_list]\n",
    "    # eliminamos las palabras de longitud menor a 2.\n",
    "    as_list = [x for x in as_list if len(x) > 1]\n",
    "    # eliminamos las stopwords.\n",
    "    as_list = [x for x in as_list if x not in mystopwords]\n",
    "    tokens = as_list\n",
    "\n",
    "    return tokens"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:34:45.218099Z",
     "start_time": "2025-05-25T17:34:45.209301Z"
    }
   },
   "source": [
    "# Aplicamos 2 procesos extras de limpieza:\n",
    "# 1. Eliminamos los caracteres repetidos (ej: \"loooove\" -> \"loove\").\n",
    "# 2. Lematizamos las palabras (ej: \"running\" -> \"run\").\n",
    "# Definimos una lista de palabras que no queremos lematizar.\n",
    "# Esto es útil para evitar lematizar pronombres y otras palabras que no cambian su forma.\n",
    "do_not_lemmatize = {\n",
    "    'i', 'me', 'my', 'mine', 'you', 'your', 'yours',\n",
    "    'he', 'him', 'his', 'she', 'her', 'hers',\n",
    "    'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs',\n",
    "    'it', 'its'\n",
    "}\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# Función para eliminar caracteres repetidos.\n",
    "def reduce_repeated_chars(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # eliminamos caracteres repetidos.\n",
    "    reduced = [reduce_repeated_chars(word) for word in doc]\n",
    "    # inicializamos el lematizador.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # obtenemos las etiquetas de parte de la oración.\n",
    "    pos_tags = pos_tag(reduced)\n",
    "    # lematizamos las palabras según su etiqueta.\n",
    "    lemmatized = []\n",
    "    for word, tag in pos_tags:\n",
    "        lower_word = word.lower()\n",
    "        if lower_word in do_not_lemmatize:\n",
    "            lemmatized.append(lower_word)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "            lemmatized.append(lemma)\n",
    "\n",
    "    tokens = lemmatized\n",
    "\n",
    "    return tokens"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7jlQuoI2o33T",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:34:47.682698Z",
     "start_time": "2025-05-25T17:34:45.758618Z"
    }
   },
   "source": [
    "# Aplicamos el proceso de limpieza y tokenización:\n",
    "Xcleantok = [clean_tok(x) for x in X]\n",
    "\n",
    "# Aplicamos el proceso de limpieza/normalización adicionales:\n",
    "Xclean = [clean_doc(x) for x in Xcleantok]  "
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:34:48.324721Z",
     "start_time": "2025-05-25T17:34:48.316538Z"
    }
   },
   "source": "Xclean[0:5] # Veamos los primeros registros para ver cómo va quedando el resultado.",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no', 'way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem'],\n",
       " ['mic', 'great']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:34:49.187232Z",
     "start_time": "2025-05-25T17:34:49.182497Z"
    }
   },
   "source": [
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygchEdcKqIzU"
   },
   "source": [
    "# **Pregunta - 3:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wEIOkkl9Dot"
   },
   "source": [
    "\n",
    "Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
    "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b0SAcYdq9X0w",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:35:00.259396Z",
     "start_time": "2025-05-25T17:35:00.247151Z"
    }
   },
   "source": [
    "# ************* Inicia la sección de agregar código:*****************************\n",
    "\n",
    "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1) \n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
    "\n",
    "# *********** Termina la sección de agregar código *************\n",
    "\n",
    "# verificamos las dimensiones obtenidas:\n",
    "print('X,y Train:', len(x_train), len(y_train))\n",
    "print('X,y Val:', len(x_val), len(y_val))\n",
    "print('X,y Test', len(x_test), len(y_test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y Train: 2100 2100\n",
      "X,y Val: 450 450\n",
      "X,y Test 450 450\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qjKoEqiqBN1"
   },
   "source": [
    "# **Pregunta - 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jENsKiN99r3F"
   },
   "source": [
    "\n",
    "\n",
    "Construye tu vocabulario a continuación\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TzJntmLPqPqC",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:35:14.537415Z",
     "start_time": "2025-05-25T17:35:14.526427Z"
    }
   },
   "source": [
    "# a.\tUsa el conjunto de entrenamiento para generar tu vocabulario\n",
    "#     con un tamaño que consideres adecuado:\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "midiccionario = Counter()    \n",
    "\n",
    "for k in range(len(x_train)):\n",
    "  midiccionario.update(x_train[k])\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yTDZ0Rr86CUP",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:35:18.887779Z",
     "start_time": "2025-05-25T17:35:18.880743Z"
    }
   },
   "source": [
    "# b.\tIndica el tamaño del vocabulario generado.\n",
    "\n",
    "print('Longitud del vocabulario generado:')\n",
    "\n",
    "# ******* Inicia la sección de agregar código: ***********\n",
    "\n",
    "print('Longitud:', len(midiccionario))  \n",
    "print('\\n(word,frequency):') \n",
    "print(midiccionario.most_common(10)) \n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario generado:\n",
      "Longitud: 3421\n",
      "\n",
      "(word,frequency):\n",
      "[('not', 218), ('good', 178), ('great', 141), ('movie', 140), ('phone', 134), ('film', 130), ('work', 113), ('time', 101), ('one', 100), ('like', 98)]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:35:33.011117Z",
     "start_time": "2025-05-25T17:35:33.003635Z"
    }
   },
   "source": [
    "# Filtramos el vocabulario para quedarnos con las palabras que aparecen al menos 3 veces en el conjunto de entrenamiento.\n",
    "min_freq = 3\n",
    "\n",
    "# filtramos el diccionario.\n",
    "midicc = {word: freq for word, freq in midiccionario.items() if freq >= min_freq}\n",
    "\n",
    "print('Nueva longitud del nuevo vocabulario:', len(midicc))\n",
    "print(list(midicc.items())[0:5])     # veamos algunos elementos del diccionario."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva longitud del nuevo vocabulario: 933\n",
      "[('star', 18), ('don', 56), ('much', 39), ('good', 178), ('people', 23)]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDa4EhTqrw15"
   },
   "source": [
    "c.\t¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?\n",
    "\n",
    "\n",
    "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
    "\n",
    "1. Evitar data leakage: Si se incluye información de los conjuntos de validación o prueba al construir el vocabulario, se corre el riesgo de que el modelo aprenda patrones específicos de esos conjuntos, lo que puede llevar a un sobreajuste y a una evaluación sesgada del rendimiento del modelo.\n",
    "2. Evaluación realista: Al construir el vocabulario solo con el conjunto de entrenamiento, se asegura que la evaluación del modelo en los conjuntos de validación y prueba sea más representativa de su rendimiento en datos no vistos.\n",
    "3. Generalización: Un vocabulario construido únicamente a partir del conjunto de entrenamiento ayuda a garantizar que el modelo se generalice mejor a nuevos datos, ya que no está influenciado por las características específicas de los conjuntos de validación o prueba.\n",
    "\n",
    "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ykjxQI3rpxx",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:36:08.165272Z",
     "start_time": "2025-05-25T17:36:08.153134Z"
    }
   },
   "source": [
    "# d.\tCon el vocabulario generado, filtra los conjuntos de entrenamiento,\n",
    "#     validación y prueba para que todos los comentarios usen solamente las\n",
    "#     palabras de este vocabulario.\n",
    "#     Llamar train_x, val_x y test_x a estos tres conjuntos.\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "train_x = []\n",
    "for ss in x_train:\n",
    "  train_x.append([w for w in ss if w in midicc])\n",
    "\n",
    "val_x = []\n",
    "for ss in x_val:\n",
    "  val_x.append([w for w in ss if w in midicc])\n",
    "\n",
    "test_x = []\n",
    "for ss in x_test:\n",
    "  test_x.append([w for w in ss if w in midicc])\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iYF2RGuPtQTC",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:36:12.309763Z",
     "start_time": "2025-05-25T17:36:12.303759Z"
    }
   },
   "source": [
    "# Vemos el resultado de los primeros comentarios del conjunto de entrenamiento:\n",
    "for ss in train_x[0:5]:\n",
    "  print(ss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['star', 'don', 'much', 'good', 'people', 'like', 'ed', 'waste']\n",
      "['special', 'suck']\n",
      "['pay', 'bill', 'not', 'tip', 'felt', 'server', 'terrible', 'job']\n",
      "['call', 'cook', 'steak', 'don', 'understand']\n",
      "['however', 'keypad', 'tinny', 'sometimes', 'wrong', 'button']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS0Hxj25vTWh"
   },
   "source": [
    "# **Pregunta - 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnHHAza5_P5Z"
   },
   "source": [
    "\n",
    "#### **Incluye aquí un resumen de las características y diferencias que tiene al menos los tres modelos de OpenAI indicados: \"text-embedding-3-small\", \"text-embedding-3-large\" y \"text-embedding-ada-002\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTI9xSgF_Xc8"
   },
   "source": [
    "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
    "\n",
    "##### ***Comparativa de modelos de OpenAI para embeddings de texto:***\n",
    "| Característica                          | `text-embedding-ada-002`               | `text-embedding-3-small`                                 | `text-embedding-3-large`                     |\n",
    "| --------------------------------------- | -------------------------------------- | -------------------------------------------------------- | -------------------------------------------- |\n",
    "| **Lanzamiento**                         | 2022                                   | Enero 2024                                               | Enero 2024                                   |\n",
    "| **Generación**                          | 2ª generación                          | 3ª generación                                            | 3ª generación                                |\n",
    "| **Dimensión del vector**                | Fija: 1536                             | Ajustable hasta 1536                                     | Ajustable hasta 3072                         |\n",
    "| **Tokens de entrada**                   | 8191                                   | 8191                                                     | 8191                                         |\n",
    "| **Precio por 1,000 tokens**             | \\$0.0001                               | \\$0.00002                                                | \\$0.00013                                    |\n",
    "| **Optimización**                        | General                                | Precisión/costo                                          | Máxima precisión                             |\n",
    "| **Multilingüe**                         | Sí (limitado)                          | Sí (mejorado)                                            | Sí (superior)                                |\n",
    "| **Soporte para reducción de dimensión** | No                                     | Sí                                                       | Sí                                           |\n",
    "| **Benchmark MIRACL (multilingüe)**      | 31.4%                                  | 44.0%                                                    | 54.9%                                        |\n",
    "| **Benchmark MTEB (tareas variadas)**    | 61.0%                                  | 62.3%                                                    | 64.6%                                        |\n",
    "| **Casos de uso ideales**                | Búsqueda general, clasificación simple | Análisis de sentimientos, motores de búsqueda económicos | Recomendaciones, búsqueda semántica avanzada |\n",
    "| **Ventaja clave**                       | Simplicidad y disponibilidad           | Balance costo/rendimiento                                | Precisión máxima y multilingüismo            |\n",
    "| **Desventaja**                          | Menor precisión                        | Menor precisión que `3-large`                            | Más caro                                     |\n",
    "\n",
    "\n",
    "##### ***Cuadro Comparativo: Ventajas, Desventajas y Casos de Uso***\n",
    "| Modelo                     | Ventajas                                                                                                            | Desventajas                                                                                | Descripción / Casos de Uso Ideales                                                                                        |\n",
    "| -------------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **text-embedding-ada-002** | ✅ Bajo costo<br>✅ Dimensión razonable (1536)<br>✅ Bien probado en producción                                        | ❌ Menor precisión<br>❌ No permite reducir dimensiones<br>❌ Superado por modelos más nuevos | Ideal para prototipos, POCs y aplicaciones de bajo presupuesto que requieran embeddings decentes sin máxima precisión.    |\n",
    "| **text-embedding-3-small** | ✅ Excelente relación costo/rendimiento<br>✅ Soporte para reducción de dimensión<br>✅ Precisión superior a `ada-002` | ❌ No alcanza el rendimiento máximo del modelo `3-large`                                    | Óptimo para la mayoría de aplicaciones productivas: clasificación, búsqueda semántica, análisis de sentimientos, etc.     |\n",
    "| **text-embedding-3-large** | ✅ Mayor precisión<br>✅ Excelente rendimiento multilingüe<br>✅ Ideal para tareas complejas                           | ❌ Costo más alto<br>❌ Mayor dimensión base                                                 | Recomendado para motores de búsqueda críticos, recomendadores, RAG, NLP multilingüe y sistemas de atención al cliente AI. |\n",
    "\n",
    "#### ***Recomendaciones de Uso***\n",
    "- **text-embedding-3-large**: Para tareas que requieren la mayor precisión y soporte multilingüe avanzado, como motores de búsqueda semántica de alta calidad y sistemas de recomendación complejos.\n",
    "    - Usa 3-large si: tu proyecto es crítico en precisión, involucra múltiples idiomas o necesitas máximo rendimiento en tareas semánticas.\n",
    "\n",
    "- **text-embedding-3-small**: Para aplicaciones que buscan un equilibrio entre rendimiento y costo, como análisis de sentimientos, clasificación de contenido y búsqueda semántica eficiente.\n",
    "    - Usa 3-small si: buscas precisión alta a bajo costo, con buena flexibilidad dimensional.\n",
    "\n",
    "- **text-embedding-ada-002**: Para proyectos existentes que ya utilizan este modelo o cuando se requiere una solución probada y rentable, aunque con menor rendimiento en comparación con los modelos más recientes.\n",
    "    - Usa ada-002 si: tu prioridad es el costo más bajo y no necesitas la mejor precisión.\n",
    "\n",
    "### ++++++++ Termina la sección de agregar texto: +++++++++++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToqRl7fT_fn2"
   },
   "source": [
    "# **Pregunta - 6:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKqQk03oqoOD"
   },
   "source": [
    "#### **Diccionario clave-valor de palabras del diccionario y vectores embebidos.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UdK-jMfLxHLY",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:40:29.963011Z",
     "start_time": "2025-05-25T17:40:29.851592Z"
    }
   },
   "source": [
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "embedding_path = \"embedding_dict.pkl\"\n",
    "\n",
    "# Si el archivo existe, lo cargamos. Si no, lo generamos y guardamos.\n",
    "if os.path.exists(embedding_path):\n",
    "    with open(embedding_path, \"rb\") as f:\n",
    "        embedding_dict = pickle.load(f)\n",
    "    print(\"Embeddings cargados desde archivo.\")\n",
    "else:\n",
    "    words = list(midicc.keys())\n",
    "    embedding_dict = {}\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            response = openai.embeddings.create(\n",
    "                input=word,\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            embedding_dict[word] = embedding\n",
    "            i += 1\n",
    "            # Imprimimos el progreso cada 100 palabras\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Progreso: {i}/{len(words)} palabras procesadas.\")\n",
    "            time.sleep(0.1)  # Para evitar rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Error con la palabra '{word}': {e}\")\n",
    "    # Guardamos el diccionario en un archivo pickle\n",
    "    with open(embedding_path, \"wb\") as f:\n",
    "        pickle.dump(embedding_dict, f)\n",
    "    print(\"Embeddings generados y guardados en archivo.\")\n",
    "\n",
    "# Ahora embedding_dict tiene la estructura {word: embedding_vector}\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cargados desde archivo.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:40:44.608755Z",
     "start_time": "2025-05-25T17:40:44.601755Z"
    }
   },
   "source": [
    "print(\"Tamaño del diccionario de embeddings:\", len(embedding_dict))\n",
    "print(\"Primeras 5 palabras y sus embeddings:\")\n",
    "for word, embedding in list(embedding_dict.items())[:5]:\n",
    "    print(f\"{word}: {embedding[:5]}...\")  # Mostramos solo los primeros 5 valores del embedding"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del diccionario de embeddings: 933\n",
      "Primeras 5 palabras y sus embeddings:\n",
      "star: [-0.013770227320492268, -0.012703923508524895, -0.020466158166527748, -0.02024831250309944, -0.013804624788463116]...\n",
      "don: [0.020259618759155273, -0.03061453439295292, 0.011888456530869007, 0.012978818267583847, 0.02128666825592518]...\n",
      "much: [0.030653979629278183, -0.03788820654153824, 0.010267253965139389, 0.052722591906785965, -0.0006658936617895961]...\n",
      "good: [0.0023704110644757748, 0.006724473088979721, -0.019609607756137848, 0.02746170200407505, 0.03913670778274536]...\n",
      "people: [0.04156694561243057, -0.03560338169336319, 0.007150345481932163, 0.07280891388654709, 0.038362644612789154]...\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4S7q0yR0Mpi"
   },
   "source": [
    "# **Pregunta - 7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyeOrkoaC1eq"
   },
   "source": [
    "\n",
    "\n",
    "Generamos los vectores embebidos a partir de los conjuntos de entrenamiento, validación y prueba.\n",
    "\n",
    "Los llamaremos trainEmb, valEmb y testEmb, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wnfQpkxg0Usq",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:42:05.984395Z",
     "start_time": "2025-05-25T17:42:04.048590Z"
    }
   },
   "source": [
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "def comment_to_embedding(comment, embedding_dict, dim):\n",
    "    vectors = [embedding_dict[w] for w in comment if w in embedding_dict]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)  # O puedes usar np.nan o alguna otra estrategia\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "# Obtén la dimensión de los embeddings\n",
    "embedding_dim = len(next(iter(embedding_dict.values())))\n",
    "\n",
    "trainEmb = np.array([comment_to_embedding(comment, embedding_dict, embedding_dim) for comment in train_x])\n",
    "valEmb = np.array([comment_to_embedding(comment, embedding_dict, embedding_dim) for comment in val_x])\n",
    "testEmb = np.array([comment_to_embedding(comment, embedding_dict, embedding_dim) for comment in test_x])\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J3BBF96D0N8Z",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:42:13.967144Z",
     "start_time": "2025-05-25T17:42:13.960722Z"
    }
   },
   "source": [
    "# Veamos las dimensiones de cada conjunto embebido:\n",
    "print(\"Train-Emb:\", trainEmb.shape)\n",
    "print(\"Val-Emb:\", valEmb.shape)\n",
    "print(\"Test-Emb:\", testEmb.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Emb: (2100, 1536)\n",
      "Val-Emb: (450, 1536)\n",
      "Test-Emb: (450, 1536)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pibp1LA91CP_"
   },
   "source": [
    "# **Pregunta - 8:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxC9K0VnGOwG"
   },
   "source": [
    "\n",
    "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños.\n",
    "\n",
    "Compara los resultados con los de la semana anterior."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ycwjD8ztGOL7",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:42:35.411561Z",
     "start_time": "2025-05-25T17:42:35.240700Z"
    }
   },
   "source": [
    "# REGRESIÓN LOGÍSTICA:\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Regresión Logística\n",
    "modeloLR = LogisticRegression(max_iter=1000, random_state=1)\n",
    "modeloLR.fit(trainEmb, y_train)\n",
    "\n",
    "print('LR: Train-accuracy: %.2f%%' % (100 * modeloLR.score(trainEmb, y_train)))\n",
    "print('LR: Val-accuracy: %.2f%%' % (100 * modeloLR.score(valEmb, y_val)))\n",
    "\n",
    "# Predicciones y reporte\n",
    "y_pred_train_lr = modeloLR.predict(trainEmb)\n",
    "y_pred_val_lr = modeloLR.predict(valEmb)\n",
    "print(\"\\nClassification Report (Train - LR):\")\n",
    "print(classification_report(y_train, y_pred_train_lr))\n",
    "print(\"\\nClassification Report (Validation - LR):\")\n",
    "print(classification_report(y_val, y_pred_val_lr))\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: Train-accuracy: 82.76%\n",
      "LR: Val-accuracy: 82.44%\n",
      "\n",
      "Classification Report (Train - LR):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84      1068\n",
      "           1       0.85      0.79      0.82      1032\n",
      "\n",
      "    accuracy                           0.83      2100\n",
      "   macro avg       0.83      0.83      0.83      2100\n",
      "weighted avg       0.83      0.83      0.83      2100\n",
      "\n",
      "\n",
      "Classification Report (Validation - LR):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       216\n",
      "           1       0.85      0.81      0.83       234\n",
      "\n",
      "    accuracy                           0.82       450\n",
      "   macro avg       0.82      0.83      0.82       450\n",
      "weighted avg       0.83      0.82      0.82       450\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N4n70GHW0sl3",
    "ExecuteTime": {
     "end_time": "2025-05-25T17:42:52.773514Z",
     "start_time": "2025-05-25T17:42:51.481209Z"
    }
   },
   "source": [
    "# BOSQUE ALEATORIO (Random Forest):\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Bosque Aleatorio\n",
    "modeloRF = RandomForestClassifier(\n",
    "    n_estimators=30,  # Menos árboles\n",
    "    max_depth=3,  # Árboles más superficiales\n",
    "    min_samples_split=20,  # Más muestras para dividir un nodo\n",
    "    min_samples_leaf=10,  # Más muestras en cada hoja\n",
    "    max_features='sqrt',  # Menos características por división\n",
    "    class_weight='balanced',  # Penalización para clases desbalanceadas\n",
    "    random_state=1\n",
    ")\n",
    "modeloRF.fit(trainEmb, y_train)\n",
    "\n",
    "print('\\nRF: Train-accuracy: %.2f%%' % (100 * modeloRF.score(trainEmb, y_train)))\n",
    "print('RF: Val-accuracy: %.2f%%' % (100 * modeloRF.score(valEmb, y_val)))\n",
    "\n",
    "# Predicciones y reporte\n",
    "y_pred_train_rf = modeloRF.predict(trainEmb)\n",
    "y_pred_val_rf = modeloRF.predict(valEmb)\n",
    "print(\"\\nClassification Report (Train - RF):\")\n",
    "print(classification_report(y_train, y_pred_train_rf))\n",
    "print(\"\\nClassification Report (Validation - RF):\")\n",
    "print(classification_report(y_val, y_pred_val_rf))\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF: Train-accuracy: 83.48%\n",
      "RF: Val-accuracy: 80.00%\n",
      "\n",
      "Classification Report (Train - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84      1068\n",
      "           1       0.87      0.78      0.82      1032\n",
      "\n",
      "    accuracy                           0.83      2100\n",
      "   macro avg       0.84      0.83      0.83      2100\n",
      "weighted avg       0.84      0.83      0.83      2100\n",
      "\n",
      "\n",
      "Classification Report (Validation - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80       216\n",
      "           1       0.83      0.77      0.80       234\n",
      "\n",
      "    accuracy                           0.80       450\n",
      "   macro avg       0.80      0.80      0.80       450\n",
      "weighted avg       0.80      0.80      0.80       450\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:51:28.886842Z",
     "start_time": "2025-05-25T17:51:20.960572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Máquina de vectores de soporte (Support Vector Machine - Classifier):\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Máquina de vectores de soporte\n",
    "modeloSVM = svm.SVC(kernel='linear')\n",
    "modeloSVM.fit(trainEmb, y_train)\n",
    "\n",
    "print('\\nRF: Train-accuracy: %.2f%%' % (100 * modeloSVM.score(trainEmb, y_train)))\n",
    "print('RF: Val-accuracy: %.2f%%' % (100 * modeloSVM.score(valEmb, y_val)))\n",
    "\n",
    "# Predicciones y reporte\n",
    "y_pred_train_rf = modeloSVM.predict(trainEmb)\n",
    "y_pred_val_rf = modeloSVM.predict(valEmb)\n",
    "print(\"\\nClassification Report (Train - RF):\")\n",
    "print(classification_report(y_train, y_pred_train_rf))\n",
    "print(\"\\nClassification Report (Validation - RF):\")\n",
    "print(classification_report(y_val, y_pred_val_rf))\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF: Train-accuracy: 84.62%\n",
      "RF: Val-accuracy: 83.56%\n",
      "\n",
      "Classification Report (Train - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      1068\n",
      "           1       0.87      0.81      0.84      1032\n",
      "\n",
      "    accuracy                           0.85      2100\n",
      "   macro avg       0.85      0.85      0.85      2100\n",
      "weighted avg       0.85      0.85      0.85      2100\n",
      "\n",
      "\n",
      "Classification Report (Validation - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.83       216\n",
      "           1       0.87      0.81      0.84       234\n",
      "\n",
      "    accuracy                           0.84       450\n",
      "   macro avg       0.84      0.84      0.84       450\n",
      "weighted avg       0.84      0.84      0.84       450\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDIiSHvg0_hm"
   },
   "source": [
    "# **Pregunta - 9:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJJtALGZHrGk"
   },
   "source": [
    "\n",
    "\n",
    "Reporte del mejor modelo con el conjunto de Prueba (Test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETv4VLjP1GYt"
   },
   "outputs": [],
   "source": [
    "# ******* Inlcuye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "\n",
    "None\n",
    "\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbhBUBKJp1MB"
   },
   "source": [
    "# **Pregunta - 10:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zPSi-H7p6ga"
   },
   "outputs": [],
   "source": "# Incluye todas las líneas de código y celdas que consideres adecuadas para este ejercicio."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:50:55.558305Z",
     "start_time": "2025-05-26T02:50:55.545539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definimos una función para tokenizar el texto sin limpiar.\n",
    "def tokenize(doc):\n",
    "    # Separamos el texto en tokens.\n",
    "    tokens = doc.split()\n",
    "    return tokens"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:51:00.154954Z",
     "start_time": "2025-05-26T02:51:00.143297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aplicamos solo proceso de tokenización sin limpieza:\n",
    "X_tok = [tokenize(x) for x in X]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:53:38.054917Z",
     "start_time": "2025-05-26T02:53:38.039674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_orig_train, x_orig_val_and_test, y_orig_train, y_orig_val_and_test = train_test_split(X_tok, Y, train_size=.70, shuffle=True, random_state=1)\n",
    "x_orig_val, x_orig_test, y_orig_val, y_orig_test = train_test_split(x_orig_val_and_test, y_orig_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
    "\n",
    "# verificamos las dimensiones obtenidas:\n",
    "print('X,y Train:', len(x_orig_train), len(y_orig_train))\n",
    "print('X,y Val:', len(x_orig_val), len(y_orig_val))\n",
    "print('X,y Test', len(x_orig_test), len(y_orig_test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y Train: 2100 2100\n",
      "X,y Val: 450 450\n",
      "X,y Test 450 450\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:56:24.816875Z",
     "start_time": "2025-05-26T02:56:24.791670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "midiccionario_orig = Counter()\n",
    "\n",
    "for k in range(len(x_orig_train)):\n",
    "    midiccionario_orig.update(x_orig_train[k])\n",
    "\n",
    "print('Longitud del vocabulario generado:', len(midiccionario_orig))\n",
    "print(midiccionario_orig.most_common(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario generado: 6212\n",
      "[('the', 1016), ('and', 732), ('a', 590), ('I', 581), ('is', 522), ('to', 463), ('of', 413), ('was', 403), ('The', 328), ('this', 311)]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "a. Realiza la transformación a vectores embebidos de todos los 3000 comentarios tal como están \n",
    "dados en los archivos. Selecciona el modelo de vector embebido que consideres más \n",
    "adecuado. Indica la cantidad de tokens de OpenAI utilizados en el proceso."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:40:30.026428Z",
     "start_time": "2025-05-26T02:58:19.333373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_orig_path = \"embeddings_orig_dict.pkl\"\n",
    "\n",
    "# Si el archivo existe, lo cargamos. Si no, lo generamos y guardamos.\n",
    "if os.path.exists(embeddings_orig_path):\n",
    "    with open(embeddings_orig_path, \"rb\") as f:\n",
    "        embeddings_orig_dict = pickle.load(f)\n",
    "    print(\"Embeddings cargados desde archivo.\")\n",
    "else:\n",
    "    words = list(midiccionario_orig.keys())\n",
    "    embeddings_orig_dict = {}\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            response = openai.embeddings.create(\n",
    "                input=word,\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            embeddings_orig_dict[word] = response.data[0].embedding\n",
    "            i += 1\n",
    "            # Imprimimos el progreso cada 100 palabras\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Progreso: {i}/{len(words)} palabras procesadas.\")\n",
    "            time.sleep(0.1)  # Para evitar rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Error al generar vector para la palabra '{word}': {e}\")\n",
    "    \n",
    "    # Guardamos el diccionario en un archivo pickle\n",
    "    with open(embeddings_orig_path, \"wb\") as f:\n",
    "        pickle.dump(embeddings_orig_dict, f)\n",
    "    print(\"Embeddings generados y guardados en archivo.\")\n",
    "\n",
    "# Ahora embeddings_orig_dict tiene la estructura {word: embedding_vector}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso: 100/6212 palabras procesadas.\n",
      "Progreso: 200/6212 palabras procesadas.\n",
      "Progreso: 300/6212 palabras procesadas.\n",
      "Progreso: 400/6212 palabras procesadas.\n",
      "Progreso: 500/6212 palabras procesadas.\n",
      "Progreso: 600/6212 palabras procesadas.\n",
      "Progreso: 700/6212 palabras procesadas.\n",
      "Progreso: 800/6212 palabras procesadas.\n",
      "Progreso: 900/6212 palabras procesadas.\n",
      "Progreso: 1000/6212 palabras procesadas.\n",
      "Progreso: 1100/6212 palabras procesadas.\n",
      "Progreso: 1200/6212 palabras procesadas.\n",
      "Progreso: 1300/6212 palabras procesadas.\n",
      "Progreso: 1400/6212 palabras procesadas.\n",
      "Progreso: 1500/6212 palabras procesadas.\n",
      "Progreso: 1600/6212 palabras procesadas.\n",
      "Progreso: 1700/6212 palabras procesadas.\n",
      "Progreso: 1800/6212 palabras procesadas.\n",
      "Progreso: 1900/6212 palabras procesadas.\n",
      "Progreso: 2000/6212 palabras procesadas.\n",
      "Progreso: 2100/6212 palabras procesadas.\n",
      "Progreso: 2200/6212 palabras procesadas.\n",
      "Progreso: 2300/6212 palabras procesadas.\n",
      "Progreso: 2400/6212 palabras procesadas.\n",
      "Progreso: 2500/6212 palabras procesadas.\n",
      "Progreso: 2600/6212 palabras procesadas.\n",
      "Progreso: 2700/6212 palabras procesadas.\n",
      "Progreso: 2800/6212 palabras procesadas.\n",
      "Progreso: 2900/6212 palabras procesadas.\n",
      "Progreso: 3000/6212 palabras procesadas.\n",
      "Progreso: 3100/6212 palabras procesadas.\n",
      "Progreso: 3200/6212 palabras procesadas.\n",
      "Progreso: 3300/6212 palabras procesadas.\n",
      "Progreso: 3400/6212 palabras procesadas.\n",
      "Progreso: 3500/6212 palabras procesadas.\n",
      "Progreso: 3600/6212 palabras procesadas.\n",
      "Progreso: 3700/6212 palabras procesadas.\n",
      "Progreso: 3800/6212 palabras procesadas.\n",
      "Progreso: 3900/6212 palabras procesadas.\n",
      "Progreso: 4000/6212 palabras procesadas.\n",
      "Progreso: 4100/6212 palabras procesadas.\n",
      "Progreso: 4200/6212 palabras procesadas.\n",
      "Progreso: 4300/6212 palabras procesadas.\n",
      "Progreso: 4400/6212 palabras procesadas.\n",
      "Progreso: 4500/6212 palabras procesadas.\n",
      "Progreso: 4600/6212 palabras procesadas.\n",
      "Progreso: 4700/6212 palabras procesadas.\n",
      "Progreso: 4800/6212 palabras procesadas.\n",
      "Progreso: 4900/6212 palabras procesadas.\n",
      "Progreso: 5000/6212 palabras procesadas.\n",
      "Progreso: 5100/6212 palabras procesadas.\n",
      "Progreso: 5200/6212 palabras procesadas.\n",
      "Progreso: 5300/6212 palabras procesadas.\n",
      "Progreso: 5400/6212 palabras procesadas.\n",
      "Progreso: 5500/6212 palabras procesadas.\n",
      "Progreso: 5600/6212 palabras procesadas.\n",
      "Progreso: 5700/6212 palabras procesadas.\n",
      "Progreso: 5800/6212 palabras procesadas.\n",
      "Progreso: 5900/6212 palabras procesadas.\n",
      "Progreso: 6000/6212 palabras procesadas.\n",
      "Progreso: 6100/6212 palabras procesadas.\n",
      "Progreso: 6200/6212 palabras procesadas.\n",
      "Embeddings generados y guardados en archivo.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "b. Realiza una partición en Train-Val-Test del 70%, 15% y 15%, respectivamente. Usa la misma \n",
    "semilla que utilizaste en el ejercicio 3, para la partición. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:49:04.443269Z",
     "start_time": "2025-05-26T03:49:04.434311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def comment_to_embedding(comment, embedding_dict, dim):\n",
    "    vectors = [embedding_dict[w] for w in comment if w in embedding_dict]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)  # O puedes usar np.nan o alguna otra estrategia\n",
    "    return np.mean(vectors, axis=0)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:49:10.475306Z",
     "start_time": "2025-05-26T03:49:06.271458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtén la dimensión de los embeddings\n",
    "embeddings_orig_dim = len(next(iter(embeddings_orig_dict.values())))\n",
    "\n",
    "train_orig_emb = np.array([comment_to_embedding(comment, embeddings_orig_dict, embeddings_orig_dim) for comment in x_orig_train])\n",
    "val_orig_emb = np.array([comment_to_embedding(comment, embeddings_orig_dict, embeddings_orig_dim) for comment in x_orig_val])\n",
    "test_orig_emb = np.array([comment_to_embedding(comment, embeddings_orig_dict, embeddings_orig_dim) for comment in x_orig_test])"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T03:49:12.855200Z",
     "start_time": "2025-05-26T03:49:12.840865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Veamos las dimensiones de cada conjunto embebido:\n",
    "print(\"Train-Emb:\", train_orig_emb.shape)\n",
    "print(\"Val-Emb:\", val_orig_emb.shape)\n",
    "print(\"Test-Emb:\", test_orig_emb.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Emb: (2100, 1536)\n",
      "Val-Emb: (450, 1536)\n",
      "Test-Emb: (450, 1536)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "c. Utiliza los modelos de regresión logística y bosque aleatorio (random forest) para este \n",
    "problema de clasificación. Para cada modelo muestra el valor de la exactitud (accuracy) y el \n",
    "reporte de sklearn dado por la función classification_report(). Verifica que no estén \n",
    "sobreentrenados y compara tus resultados con los que obtuviste en la primera parte. Puedes \n",
    "incluir algún otro modelo de machine learning si lo consideras adecuado."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T04:03:02.592313Z",
     "start_time": "2025-05-26T04:03:02.488546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# REGRESIÓN LOGÍSTICA:\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Regresión Logística\n",
    "modeloLR_orig = LogisticRegression(max_iter=1000, random_state=1)\n",
    "modeloLR_orig.fit(train_orig_emb, y_orig_train)\n",
    "\n",
    "print('LR: Train-accuracy: %.2f%%' % (100 * modeloLR_orig.score(train_orig_emb, y_orig_train)))\n",
    "print('LR: Val-accuracy: %.2f%%' % (100 * modeloLR_orig.score(val_orig_emb, y_orig_val)))\n",
    "\n",
    "# Predicciones y reporte\n",
    "y_pred_train_lr = modeloLR_orig.predict(train_orig_emb)\n",
    "y_pred_val_lr = modeloLR_orig.predict(val_orig_emb)\n",
    "print(\"\\nClassification Report (Train - LR):\")\n",
    "print(classification_report(y_orig_train, y_pred_train_lr))\n",
    "print(\"\\nClassification Report (Validation - LR):\")\n",
    "print(classification_report(y_orig_val, y_pred_val_lr))\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: Train-accuracy: 86.43%\n",
      "LR: Val-accuracy: 80.67%\n",
      "\n",
      "Classification Report (Train - LR):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87      1068\n",
      "           1       0.90      0.81      0.85      1032\n",
      "\n",
      "    accuracy                           0.86      2100\n",
      "   macro avg       0.87      0.86      0.86      2100\n",
      "weighted avg       0.87      0.86      0.86      2100\n",
      "\n",
      "\n",
      "Classification Report (Validation - LR):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80       216\n",
      "           1       0.80      0.83      0.82       234\n",
      "\n",
      "    accuracy                           0.81       450\n",
      "   macro avg       0.81      0.81      0.81       450\n",
      "weighted avg       0.81      0.81      0.81       450\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T04:03:05.932312Z",
     "start_time": "2025-05-26T04:03:04.648893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# BOSQUE ALEATORIO (Random Forest):\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Bosque Aleatorio\n",
    "modeloRF_orig = RandomForestClassifier(\n",
    "    n_estimators=30,  # Menos árboles\n",
    "    max_depth=3,  # Árboles más superficiales\n",
    "    min_samples_split=20,  # Más muestras para dividir un nodo\n",
    "    min_samples_leaf=10,  # Más muestras en cada hoja\n",
    "    max_features='sqrt',  # Menos características por división\n",
    "    class_weight='balanced',  # Penalización para clases desbalanceadas\n",
    "    random_state=1\n",
    ")\n",
    "modeloRF_orig.fit(train_orig_emb, y_orig_train)\n",
    "\n",
    "print('\\nRF: Train-accuracy: %.2f%%' % (100 * modeloRF_orig.score(train_orig_emb, y_orig_train)))\n",
    "print('RF: Val-accuracy: %.2f%%' % (100 * modeloRF_orig.score(val_orig_emb, y_orig_val)))\n",
    "\n",
    "# Predicciones y reporte\n",
    "y_pred_train_rf = modeloRF_orig.predict(train_orig_emb)\n",
    "y_pred_val_rf = modeloRF_orig.predict(val_orig_emb)\n",
    "print(\"\\nClassification Report (Train - RF):\")\n",
    "print(classification_report(y_orig_train, y_pred_train_rf))\n",
    "print(\"\\nClassification Report (Validation - RF):\")\n",
    "print(classification_report(y_orig_val, y_pred_val_rf))\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF: Train-accuracy: 83.71%\n",
      "RF: Val-accuracy: 73.78%\n",
      "\n",
      "Classification Report (Train - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      1068\n",
      "           1       0.86      0.79      0.83      1032\n",
      "\n",
      "    accuracy                           0.84      2100\n",
      "   macro avg       0.84      0.84      0.84      2100\n",
      "weighted avg       0.84      0.84      0.84      2100\n",
      "\n",
      "\n",
      "Classification Report (Validation - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.72       216\n",
      "           1       0.73      0.78      0.76       234\n",
      "\n",
      "    accuracy                           0.74       450\n",
      "   macro avg       0.74      0.74      0.74       450\n",
      "weighted avg       0.74      0.74      0.74       450\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T04:03:16.676640Z",
     "start_time": "2025-05-26T04:03:07.819532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Máquina de vectores de soporte (Support Vector Machine - Classifier):\n",
    "\n",
    "# ******* Incluye a continuación todas las líneas de código y celdas que requieras: ***********\n",
    "\n",
    "# Máquina de vectores de soporte\n",
    "modeloSVM_orig = svm.SVC(kernel='linear')\n",
    "modeloSVM_orig.fit(train_orig_emb, y_orig_train)\n",
    "\n",
    "print('\\nRF: Train-accuracy: %.2f%%' % (100 * modeloSVM_orig.score(train_orig_emb, y_orig_train)))\n",
    "print('RF: Val-accuracy: %.2f%%' % (100 * modeloSVM_orig.score(val_orig_emb, y_orig_val)))\n",
    "\n",
    "# Predicciones y reporte\n",
    "y_pred_train_svm = modeloSVM_orig.predict(train_orig_emb)\n",
    "y_pred_val_svm = modeloSVM_orig.predict(val_orig_emb)\n",
    "print(\"\\nClassification Report (Train - RF):\")\n",
    "print(classification_report(y_orig_train, y_pred_train_svm))\n",
    "print(\"\\nClassification Report (Validation - RF):\")\n",
    "print(classification_report(y_orig_val, y_pred_val_svm))\n",
    "\n",
    "# *********** Aquí termina la sección de agregar código *************"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF: Train-accuracy: 87.05%\n",
      "RF: Val-accuracy: 79.33%\n",
      "\n",
      "Classification Report (Train - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88      1068\n",
      "           1       0.93      0.80      0.86      1032\n",
      "\n",
      "    accuracy                           0.87      2100\n",
      "   macro avg       0.88      0.87      0.87      2100\n",
      "weighted avg       0.88      0.87      0.87      2100\n",
      "\n",
      "\n",
      "Classification Report (Validation - RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       216\n",
      "           1       0.81      0.79      0.80       234\n",
      "\n",
      "    accuracy                           0.79       450\n",
      "   macro avg       0.79      0.79      0.79       450\n",
      "weighted avg       0.79      0.79      0.79       450\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCkh2WfN1MC1"
   },
   "source": [
    "# **Pregunta - 11:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ySFuDQtVuK5"
   },
   "source": [
    "\n",
    "\n",
    "Incluye tus comentarios finales de la actividad.\n",
    "\n",
    "### ++++++++ Inicia la sección de agregar texto: +++++++++++\n",
    "\n",
    "None\n",
    "\n",
    "### ++++++++ Termina la sección de agregar texto: +++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgKHmQTbWJT1"
   },
   "source": [
    "# **Fin de la Actividad de Vectores Embebidos - OpenAI**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4c34ZOnna3Gu",
    "MfZZ0stLmWJN",
    "ygchEdcKqIzU",
    "1qjKoEqiqBN1",
    "RS0Hxj25vTWh",
    "ToqRl7fT_fn2",
    "W4S7q0yR0Mpi",
    "pibp1LA91CP_",
    "WDIiSHvg0_hm",
    "NbhBUBKJp1MB",
    "YCkh2WfN1MC1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
